# Comparing Concurrency in C, C++, and Rust

Digging into operating system fundamentals means getting intimate with concurrency -- and getting intimate with concurrency means getting intimate with languages' implementations (and APIs) for managing threads, locks, condition variables, and more. Since I'm passionate about systems programming, I decided it would be essential to learn more about concurrency in three low(er)-level languages: C, C++, and Rust -- and actually, the more I learn about concurrency, the more I'm finding that it's not just essential for low-level programming, but for programming in general; in fact, it's pretty much a waste of resources not to implement concurrent methods in programming (whenever reasonable), since modern hardware typically supports multiple processors and even cores in single processors, allowing for multiple threads to run not only concurrently, but even literally in parallel.

So this week's post is going to compare what I've learned about concurrency in the above-mentioned languages. I'll explain what I've learned so far mainly to solidfy my own understanding, though maybe even for the benefit of others who are interested in exploring concurrent programming. As a disclaimer, Rust is much newer to me than C and C++ are, and I have yet to learn about unsafe programming in Rust, which (to my current understanding) is where concurrency really comes alive, but I know enough now to compare it meaningfully with the other two languages. The following might be most helpful for those who aren't familiar with Rust yet at all.

As another disclaimer, this post will only focus on **concurrent** programming -- not **parallel** or **asynchronous** programming. That's maybe a blog post for the future.

I'll mainly focus my comparisons within the contexts of the books I'm reading, which (along with their abbreviations) are:

- Operating Systems: Three Easy Pieces (OSTEP) by Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau
- Advanced Programming in the UNIX Environment (APUE), Third Edition by W. Richard Stevens
- C++ Concurrency in Action (CCIA) by Anthony Williams
- The Rust Programming Language (The Book) by Steve Klabnik, Carol Nichols, and Chris Krycho, with contributions from the Rust Community

The first two feature concurrent programming in C, which will kick off my comparison. But first, I'll talk a bit about shared concepts across all three languages (and in concurrent programming in general).

## Concurrency Concepts

Concurrency in programming is when tasks are split among multiple processes or threads. These processes/threads can carry out their tasks either literally in parallel (if they're spread out over multiple processors or cores), or virtually in parallel, i.e. by sharing CPU time in some coordinated fashion, such as (trivially) a round-robin sort of rotation. This allows for various functions to happen at the same time, instead of sequentially, improving responsiveness and potentially speeding up the time it takes to complete large tasks or sets of tasks. A common use of concurrency is in a web server, which processes multiple user requests simultaneously (imagine if we all had to wait in line to use the internet...).

At the level of the operating system, multiple programs can run concurrently as **processes**. You can have this web page open along with your desktop, your text editor, etc.; the operating system juggles all of these programs concurrently, giving them each their own virtual address spaces, and even doing quite a bit to present to each process the illusion that it, alone, is using up any memory.

This artfulness comes at a cost: between processes, the OS must perform a **context switch**, which involves saving the state of one process and loading the state of another process in its place. The reason this isn't performant is because processes don't share their address spaces with other processes; although very useful, this produces overhead that makes processes difficult to scale. The more individual tasks are carried out by individual processes, the more context switching the OS has to do, which is inefficient.

Much more performant is delegating multiple tasks to single processes and letting them concurrently accomplish those tasks. This is called **multithreading**, and the reason it's more performant is because **threads** don't each have their own virtual address spaces: they *share* the address space of the process that spawned them, so there is much less overhead switching between threads.

Again, as with most concepts in programming (and life), this is a tradeoff: multithreading is much more efficient than running multiple processes, but it's much harder to implement because the programmer (me! [and maybe you!]) has to personally manage how those threads share the address space. This involves making sure that a thread doesn't alter shared data while another thread is doing something important with that data, coordinating in what order threads must access shared data, and making sure that threads aren't stuck in a **deadlock**, which is when two threads are waiting for each other to finish what the other is doing before they can each continue their tasks; however, since each thread is waiting for the other, they are both stuck infinitely waiting.

Yikes. With great power comes great yadda yadda, but luckily, there are plenty of great tools and resources for managing threads and making sure that none of the above potential problems become more than potential. For example, C (through the POSIX API), C++, and Rust all provide users with **locks** that ensure that only one thread at a time is performing a pivotal operation on a shared resource. These locks, called **mutexes** (mut + ex = mutual exclusive), can only be held by one thread at a time; if another thread tries to hold a lock while another thread has it, it has to wait until the lock is available. For example, in C, if a `pthread_t` is unable to acquire a `pthread_mutex_t`, a **condition variable** can be passed to `pthread_cond_wait` to put the waiting thread to sleep until a thread calls `pthread_cond_signal` to satisfy the condition and wake it.

Although other similarities exist between how these three languages enable concurrency, here's where the differences start to emerge, so I'll go ahead and start with how C and POSIX do things.

## Concurrency in C

Ahh, C. What a deceptively simple language. It's my favorite to work with because it feels like I can do pretty much anything with just a couple pointers and a few headers. It feels more creative and constructive; less rote like other languages that lean heavily on their std libraries (more on that soon).

This raw(er) (sorry, I'm learning assembly and can't call C "low-level" anymore), yet elegant simplicity threatens to fly out the window once you start using the POSIX API -- which you're going to want to use when programming concurrently in C. Beyond providing high-level functions for file operations, the POSIX API contains pivotal headers for managing processes, threads, sockets, and signals.

The big one for this discussion is `pthread.h`, which includes all of those lovely variables and functions above, and more. It's not actually strictly necessary to use tools like `pthread_mutex_t` or `pthread_cond_t`; OSTEP actually walks you through considerations when designing your own lock, including implementations of test-and-set, compare-and-swap, and even load-linked and store-conditional (LL/SC to the pros out there). This is great for getting you thinking about how these concepts work in the most raw fashion, but OSTEP doesn't get you much farther than just thinking about the problem, since their implementations don't get past spin locks (which are not performant). Better to just use `pthread_mutex_lock`, unless you're really serious about [doing it like the pros](https://codebrowser.dev/glibc/glibc/nptl/pthread_mutex_lock.c.html).

The great thing about concurrency in C, which is the same thing that's great about C, is that, even with APIs, it's very simple and straightforward to define your own implementations of even the most fundamental concepts involved in concurrent programming (you really can just implement your own spin lock, and that is very nice). The not-so-great thing about concurrency in C, which is just what's not-so-great about C, is that, as with memory management, you have to be **really careful** to remember to call any accompanying freeing functions for things like `pthread_mutex_lock` and `pthread_cond_wait`. Forgetting to put these in the right places can lead to all sorts of tricky problems, such as infinite sleep or double freeing.

Luckily, C++ and Rust help out big time with this issue.

## Concurrency in C++

First of all, C++ Concurrency in Action is a very good book. It is not overly pedantic (as OSTEP can be at times) and focuses entirely on concurrency, which means you get to read almost 600 pages of what it means to program *just concurrently* in C++. This is great if you're already familiar with C++ fundamentals, but if you're not comfortable with things like smart pointers and typenames, then this book is possibly too steep for you. However, it is not only great for learning the std library's provisions for concurrency, but also excellent for thinking about concurrency in general. My main critique is that, so far, it maybe leans a little too abstract; I wish that the author had provided some exercises or something to do along the way. But if his goal was to share how the std library addresses common issues related to concurrency, then it's fantastic; I can come up with my own exercises.

The nice thing about C++ is that "resource allocation is initialization" (RAII). What this cool and awkward slogan really means is that C++ is very scope-conscious: when resources are initialized within a certain scope, such as within a function, C++ will clean them up for you when they are out of scope, including by freeing mutexes for you when the program returns from functions that initialize the locks.

Of course, as with all things, but especially with C++, this abstraction comes at the cost of knowing the correct abstractions to implement -- and, depending on your use case, there might not be just one. Calling threads and mutexes are very straightforward (`std::thread` and `std::mutex`), but at the cost of not having to call, for example, `some_mutex.lock()` and `some_mutex.unlock()`, you might have to call (variously and sometimes in tandem) `std::lock`, `std::lock_guard`, `std::scoped_lock`, and/or `std::unique_lock` -- among others. Again, which you use depends on what you're trying to achieve; `std::lock_guard` is a good default, but if you're trying to prevent deadlock or enforce a lock sequence (such as when you must always grab two or more locks for a certain operation), then you'll have to resort to either using `std::lock` with `std::lock_guard` and `std::adopt_lock` to grab both locks (at the same time -- very important!), `std::scoped_lock` as (another) abstraction for the former case, or possibly `std::unique_lock` with `std::deferred_lock` for flexible locking, such as when you need to transfer mutex ownership between scopes.

Those are only *some* of the pertinent abstractions, and they still can't help you out with things like implementing a hierarchical lock, which, although very useful and very cool, will still need to be hand-rolled -- so, in addition to knowing all of the above abstractions, you'll have to create *your own abstractions* as well! At least, in C, creating abstractions is the default; in C++, you have to know them *and* implement them.

Concurrency in C++ requires that you think *deeply* about concurrency in general -- but what if you want fast, safe, concurrent code, *without* thinking very hard about it?

## Concurrency in Rust

First of all, it's not at all a bad thing that Rust handles a lot of issues with concurrency for you; most often, this is a great thing. Secondly, very little of how Rust handles concurrency is part of the language; whereas much can be done from scratch in C, and knowing C++ pretty much just means knowing the std library, Rust's treatment of concurrency almost feels like an add-on: a very good add-on, but most of its implementations exist as third-party libraries. To be fair, Rust's ecosystem is tremendous; I like to say that the best thing about Rust is actually Cargo, and the ease with which you can add dependencies to your project is comparable to package managers for much more user-friendly languages. That said, concurrency in Rust is tricky because it offers little native support, and because thinking about concurrency in Rust usually ends up being thinking about how Rust works under the hood.

As in C++, resources in Rust are scoped, so you don't have to worry about unlocking your mutexes at the end of every function you need them. Differently from both C++ and C, mutexes are not some static entity that are grabbed when you need to lock access to a data structure: they actually *encase* the data itself. You initialize a `Mutex<T>` by passing in the data you want protected, then call lock() directly on it and dereference it to do things with the data. On the surface, this is such an intuitive improvement on the C/C++ way of doing things that it makes those languages feel old and clunky by comparison.

*Tradeoffs. Tradeoffs, everywhere.* While I really like the fact that `Mutex<T>` functions as a pointer to data, Rust's strict rules on borrowing and ownership quickly make this less intuitive in use. For example, consider trying to implement a protected counter that's incremented by multiple threads. First of all, you call `thread::spawn()` to take a **closure** (Rust's take on lambda functions) that uses the `move` keyword to take in the mutex to the counter. You call `lock()` *and* `unwrap()` on the counter because `lock()` returns a `Result<T, E>`, which is a generic that is essentially Rust's alternative to using NULL, and which either needs to have `match` cases for both the T and the E (error), or just be unwrapped to access the data type T within. *However*, this doesn't work because of Rust's move semantics: since the counter is moved into the closure, it can no longer be dereferenced outside of the closure -- so all of the multithreaded counting is futile. To fix this, you have to use a smart pointer to let the mutex have multiple owners -- except that you can't use the typical, single-threaded `std::rc::Rc` (r + c = reference counter) because it's not threadsafe, *so* you have to instead use `std::sync::Arc` (a + r + c = atomic reference counter) to call `Arc::clone(&counter)` to safely allow for multiple owners and modification of the counter.

Aside from the convoluted network of std libaries, this is not a bad implementation, per se. I actually like a lot about this, particularly that, similarly to `Mutex<T>`, `std::thread` is not inherently separate from the function it runs, the mutex it requires, and the data that the mutex protects. All of these components are very nicely interwoven: so much so that, even just by looking at the code, you can *see* that functions (closures) happen *within* a thread, and that data is both protected and accessed within this thread. This is a great model. However, as the above situation shows, it's not easy to think about *concurrency* when trying to implement it. All of the difficulties in the above situation have to do specifically with how Rust handles ownership, and the best way that the language can consolidate its own rules with concurrency is to provide libraries that help you deal with it. Again, while I love the ecosystem and think that the thread model is really intuitive, it still feels more like thinking about *concurrency in Rust* rather than thinking about *concurrency itself*, which could be an issue if Rust is your introduction to concurrency.

It's not that cut-and-dry; Rust does have `Send` and `Sync` traits that can be used for personal implementations of concurrency. These are apparently explored in the Rustonomicon, but, from skimming that section of the book, it seems to be still under development, so the best way to really make use of concurrency in Rust is to read the Reference, which seems dry, to say the least. But Rust's libraries are still very helpful for implementing concurrency, including `std::sync::mpsc` that sends and receives messages over a channel. Plus, what's really nice about learning Rust through The Book is that it's almost completely hands-on: every chapter is pretty much a walkthrough of code that it encourages you to write yourself, and it has two major projects in the form of minigrep and a full-blown web server, the latter of which is a real, *practical* project that gets you concurrently programming in real life. So again, the support that the Rust community lends to newcomers is absolutely fantastic, and though it maybe reduces thinking about concurrency, it does get you programming concurrently really fast.

## Concluding Thoughts

These aren't my definitive thoughts on concurrency in C, C++, and Rust, especially since I'm still reading the great resources I mentioned (and still unfamiliar with the dark arts of unsafe Rust). But as I'm becoming more familiar with thinking about problems of concurrency, I've been finding it really helpful to compare how different languages approach the issue. I even started looking at concurrency in Python today; as with many things in Python, it told me almost nothing about programming, but man was it fast(er)! If you're reading this, I hope you found it helpful or interesting, and if you're not reading this, then I hope you're programming concurrently instead.

Till next time!
